{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pharseus/Machine-Learning/blob/main/Naive_Bayes_w4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **03. Naive Bayes**\n",
        "\n",
        "Please copy or download this python notebook to your drive for assignment submission\n"
      ],
      "metadata": {
        "id": "e9FpHteVLbeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Required Libraries**\n",
        "For this practicum, we will need some libraries\n",
        "1. **NumPy**\n",
        "2. [**Pandas**](https://pandas.pydata.org/)\n",
        "\n",
        "    Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool\n",
        "3. **Scikit-learn**"
      ],
      "metadata": {
        "id": "yP3UUvTNMc-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import kagglehub"
      ],
      "metadata": {
        "id": "uJesIRNWMv-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv"
      ],
      "metadata": {
        "id": "9Juvagt6iWe9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Categorical Dataset and Categorical Naive Bayes**"
      ],
      "metadata": {
        "id": "RW4zoagpM5CA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset  from kaggle by using `kagglehub` for employee datasets :\n",
        "\n",
        "https://www.kaggle.com/datasets/tawfikelmetwally/employee-dataset"
      ],
      "metadata": {
        "id": "5skkZTwxhFtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **TO DO** Download employee dataset with kagglehub\n",
        "dataset_path = kagglehub.dataset_download(\"tawfikelmetwally/employee-dataset\")\n",
        "# print(dataset_path)\n"
      ],
      "metadata": {
        "id": "88tCU6qIkKCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## **TO DO** Load the data using pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data from the specified path\n",
        "employee = pd.read_csv(f\"{dataset_path}/Employee.csv\")\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify\n",
        "display(employee.head())"
      ],
      "metadata": {
        "id": "pyZuuBOafpZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding categorical data with LabelEncoder\n",
        "We can use `sklearn` library to do it. Read the documentation here [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)"
      ],
      "metadata": {
        "id": "w6yvFPbZpvOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **TO DO** Import and initialize LabelEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "enc = LabelEncoder()"
      ],
      "metadata": {
        "id": "1ShcuqtZZqfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using LabelEncoder for pandas DataFrame, we can automatically \"`apply`\" `fit_transform` function to each and every column\n",
        "\n",
        "LabelEncoder will automatically finds columns which are non numerical.\n",
        "\n",
        "See the following code :"
      ],
      "metadata": {
        "id": "_bnvaNoUb6k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the dataset.\n",
        "employee_ = employee.apply(enc.fit_transform)"
      ],
      "metadata": {
        "id": "BSKME7rVaArb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "employee_"
      ],
      "metadata": {
        "id": "Ju1ebd3MpeOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# from google.colab import sheets\n",
        "# sheet = sheets.InteractiveSheet(df=employee_)"
      ],
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "cellView": "form",
        "id": "N7ySeFnkUFCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to separate the DataFrame for the input and target. For employe dataset, the target column is `LeaveOrNot`, while the rest of the column is the input.\n",
        "\n",
        "In order to do so, we need to use\n",
        "```\n",
        "df.iloc[row_indexer, column_indexer]\n",
        "```\n",
        "for the dataset.\n",
        "\n",
        "In this example, we take all rows except the last column for the input. (We use index slicing, tutorial [here](https://www.freecodecamp.org/news/slicing-and-indexing-in-python/))\n",
        "```\n",
        "df.iloc[:,:-1]\n",
        "```\n",
        "\n",
        "and all rows and only the last column for the target\n",
        "```\n",
        "df.iloc[:,-1]\n",
        "```\n",
        "\n",
        "More tutorial on using `iloc` [here](https://www.w3schools.com/python/pandas/ref_df_iloc.asp)"
      ],
      "metadata": {
        "id": "qjbIoO71eqgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = employee_.iloc[:, :-1]\n",
        "y = employee_.iloc[:, -1]"
      ],
      "metadata": {
        "id": "i49I2_wXp4ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "EK3vlMLD80QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "ZL8zhR07eu_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we will split the dataset to be\n",
        "- 70% Train\n",
        "- 15% Val\n",
        "- 15% Val"
      ],
      "metadata": {
        "id": "XJ8piyrce7TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-Val-Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val_test, y_train, y_val_test =\\\n",
        "    train_test_split(X, y, train_size=0.7, shuffle=True)\n",
        "\n",
        "X_val, X_test, y_val, y_test =\\\n",
        "    train_test_split(X_val_test, y_val_test, train_size=0.5, shuffle=True)"
      ],
      "metadata": {
        "id": "ujDyMEhsY2fO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importing CategoricalNB**\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html#sklearn.naive_bayes.CategoricalNB"
      ],
      "metadata": {
        "id": "_RpQzfQVrH5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and initialize CategoricalNB\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "\n",
        "cnb = CategoricalNB()"
      ],
      "metadata": {
        "id": "Q_aV133oYo-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train CategoricalNB with train data\n",
        "cnb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "43wohUgwZNBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the label of train data\n",
        "pred_train = cnb.predict(X_train)"
      ],
      "metadata": {
        "id": "bR_8gv4oaOlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure the accuracy of the predicted train data\n",
        "np.sum(y_train == pred_train) / len(y_train)"
      ],
      "metadata": {
        "id": "02Yt73UolC6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_train"
      ],
      "metadata": {
        "id": "il3wX2vHsYPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure the accuracy on validation data using model.score()\n",
        "cnb.score(X_val, y_val)"
      ],
      "metadata": {
        "id": "Hz128a5zmYYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure the accuracy on test data using model.score()\n",
        "cnb.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "XARYguRorxy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Frequency Dataset and Naive Bayes Multinomial**"
      ],
      "metadata": {
        "id": "WRSy9VVnZTnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the same thing as above using MultinomialNB.\n",
        "\n",
        "Download spam dataset using `kagglehub`:\n",
        "\n",
        "https://www.kaggle.com/datasets/balaka18/email-spam-classification-dataset-csv\n",
        "\n",
        "This datasets, consists of count/frequency of word usage in an email"
      ],
      "metadata": {
        "id": "JE493rYXlHKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **TO DO** Download spam email dataset with kagglehub\n",
        "import kagglehub\n",
        "\n",
        "dataset_path = kagglehub.dataset_download(\"balaka18/email-spam-classification-dataset-csv\")\n",
        "\n",
        "print(dataset_path)\n"
      ],
      "metadata": {
        "id": "uFNDjuiEtd41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## **TO DO** Load the data using pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data from the specified path\n",
        "spam_dataset = pd.read_csv(f\"{dataset_path}/emails.csv\")\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify\n",
        "display(spam_dataset.head())"
      ],
      "metadata": {
        "id": "Fg_V4CSHsK2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***TO DO*** Split dataset into train, val, test"
      ],
      "metadata": {
        "id": "9sDA55S9sOr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val_test, y_train, y_val_test =\\\n",
        "    train_test_split(X, y, train_size=0.7, shuffle=True)\n",
        "\n",
        "X_val, X_test, y_val, y_test =\\\n",
        "    train_test_split(X_val_test, y_val_test, train_size=0.5, shuffle=True)"
      ],
      "metadata": {
        "id": "QHtfYMiLbA7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***TO DO*** **Import and initialize MultinomialNB**\n",
        "\n",
        "To use `MultinomialNB`, read the documentation:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB\n"
      ],
      "metadata": {
        "id": "83_iIFiJsgwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and initialize MultinomialNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "mnb = MultinomialNB()"
      ],
      "metadata": {
        "id": "6CDHKX7Bswk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train MultinomialNB with train data\n",
        "mnb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "WXrb2gA4swk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure the accuracy on train data using model.score()\n",
        "mnb.score(X_train, y_train, sample_weight=None)"
      ],
      "metadata": {
        "id": "yHQ35Pt_swk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure the accuracy on validation data using model.score()\n",
        "mnb.score(X_val, y_val, sample_weight=None)"
      ],
      "metadata": {
        "id": "Tdty_0bvswk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure the accuracy on test data using model.score()\n",
        "mnb.score(X_test, y_test, sample_weight=None)"
      ],
      "metadata": {
        "id": "HZsjvBdjswk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Continuous Dataset and Gaussian Naive Bayes**"
      ],
      "metadata": {
        "id": "JuBHGXhxm2Yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the same thing as above using MultinomialNB.\n",
        "\n",
        "Download spam dataset using `kagglehub`:\n",
        "\n",
        "https://www.kaggle.com/datasets/uom190346a/water-quality-and-potability"
      ],
      "metadata": {
        "id": "rr-IEdnvnftK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **TO DO** Download water quality dataset with kagglehub\n",
        "import kagglehub\n",
        "dataset_path = kagglehub.dataset_download(\"uom190346a/water-quality-and-potability\")\n",
        "# !ls \"{dataset_path}\"\n",
        "\n",
        "print(dataset_path)"
      ],
      "metadata": {
        "id": "UCqO4TwCu6vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## **TO DO** Load the data using pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data from the specified path\n",
        "water_dataset = pd.read_csv(f\"{dataset_path}/water_potability.csv\")\n",
        "\n",
        "# Display the first few rows of the DataFrame to verify\n",
        "display(water_dataset.head())"
      ],
      "metadata": {
        "id": "6YXoAJ90u6vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see that there is some missing data. We will do KNN imputation"
      ],
      "metadata": {
        "id": "3x7Yo7qCh3Is"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***TO DO*** Split dataset into train, val, test"
      ],
      "metadata": {
        "id": "XX_N4PUdtOeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = water_dataset.iloc[:, :-1]\n",
        "y = water_dataset.iloc[:, -1]\n",
        "\n",
        "X_train, X_val_test, y_train, y_val_test =\\\n",
        "    train_test_split(X, y, train_size=0.65, shuffle=True)\n",
        "# val and test, each 17,5% of data\n",
        "X_val, X_test, y_val, y_test =\\\n",
        "    train_test_split(X_val_test, y_val_test, train_size=0.5, shuffle=True)\n",
        "    #"
      ],
      "metadata": {
        "id": "cHtTDyL4noxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we've done data splitting, we will do imputation **only based on train data**\n",
        "\n",
        "such that, we will do `fit_transform` for the X_train, and only `transform` for the X_val and X_test.\n",
        "\n",
        "This is important because we will make the model learn **only from the train data**"
      ],
      "metadata": {
        "id": "DgfgBqe9iC1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# Perform KNN imputation\n",
        "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
        "X_val = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns)\n",
        "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)"
      ],
      "metadata": {
        "id": "Q1ypLIuvhd1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we check the water dataset,\n",
        "|index|ph|Hardness|Solids|Chloramines|Sulfate|Conductivity|Organic\\_carbon|Trihalomethanes|Turbidity|\n",
        "|---|---|---|---|---|---|---|---|---|---|\n",
        "|0|5\\.49597599882932|150\\.5345189469585|18333\\.632664093995|7\\.471727772298008|416\\.8885268904612|300\\.3589720105804|12\\.300193116160472|55\\.70678076355855|4\\.901038222067857|\n",
        "|1|7\\.408156691693629|164\\.37843332923484|15908\\.561870396212|11\\.208688337573191|350\\.6105483590927|360\\.81429637880143|17\\.239577381047184|60\\.1791185610558|4\\.380978304316724|\n",
        "|2|9\\.658661976447796|240\\.34882037477328|20269\\.73127975584|7\\.050728842221929|374\\.18167434281554|567\\.7842756927254|15\\.587205853529213|55\\.83596570642267|4\\.6393865551816|\n",
        "|3|7\\.790572114992737|202\\.88242945570312|17505\\.852735168726|7\\.104365818605476|310\\.53283662626154|429\\.2898954320752|17\\.958968013429|78\\.58753831483234|3\\.537040784668471|\n",
        "|4|8\\.040392261051977|200\\.63757592628696|12999\\.14480401679|6\\.696355790704156|356\\.8700406041423|394\\.01773247312406|11\\.100406898066524|43\\.62591046091927|3\\.017795963114114|\n",
        "\n",
        "You should see that the numbers varies greatly,\n",
        "- ph in range of 1-14\n",
        "- Hardness in range of hundreds\n",
        "- etc.\n",
        "\n",
        "Thus, let's use `StandardScaler` for the dataset. Again, only fit the `StandardScaler` using the `X_train`"
      ],
      "metadata": {
        "id": "f4bUiHjXidhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **TO DO** use StandardScaler to fit X_train, then transform X_train, X_val, and X_test\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "X_val = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n",
        "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
      ],
      "metadata": {
        "id": "nTcEePCRgOOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "8bytGt36gcQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***TO DO*** **Import and initialize GaussianNB**\n",
        "To use `GaussianNB`, read the documentation:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"
      ],
      "metadata": {
        "id": "xIuTzwu5T6aI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and initialize GaussianNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()"
      ],
      "metadata": {
        "id": "MKzU42UNtf1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train GaussianNB with train data\n",
        "gnb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "6ZrL8MG2tf1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure the accuracy on train data using model.score()\n",
        "gnb.score(X_train, y_train, sample_weight=None)"
      ],
      "metadata": {
        "id": "eOfBqMNwtf1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure the accuracy on validation data using model.score()\n",
        "gnb.score(X_val, y_val, sample_weight=None)"
      ],
      "metadata": {
        "id": "s7QLl8zJtf1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure the accuracy on test data using model.score()\n",
        "gnb.score(X_test, y_test, sample_weight=None)"
      ],
      "metadata": {
        "id": "McdfrXsutf1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turns out the accuracy of the model is only 60%. According to this [Notebook](https://www.kaggle.com/code/emrearslan123/water-potability-prediction#Machine-Learning-Models) it is expected"
      ],
      "metadata": {
        "id": "o9LTAULnjU_-"
      }
    }
  ]
}